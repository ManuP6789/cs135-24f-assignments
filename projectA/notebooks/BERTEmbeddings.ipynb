{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generating Embeddings with BERT (Bidirectional Encoder Representations from Transformers) \n",
    "\n",
    "### What is BERT?\n",
    "BERT is a large state-of-the-art neural network that has been trained on a large corpora of text (millions of sentences). Its applications include but are not limited to:\n",
    "\n",
    "- Sentiment analysis\n",
    "- Text classification\n",
    "- Question answering systems\n",
    " \n",
    "In this notebook, we walk through how BERT generates fixed-length embeddings (features) from a sentence. You could think of these embeddings as an alternate feature extraction technique compared to bag of words. The BERT model has 2 main components as shown below \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Tokenizer (Converting sentences into series of numerical tokens):\n",
    "\n",
    "The tokenizer in BERT is like a translator that converts sentences into a series of numerical tokens that the BERT model can understand. Specifically, it does the following:\n",
    "\n",
    "- Splits Text: It breaks down sentences into smaller pieces called tokens. These tokens can be as short as one character or as long as one word. For example, the word \"chatting\" might be split into \"chat\" and \"##ting\".\n",
    "\n",
    "- Converts Tokens to IDs: Each token has a unique ID in BERT's vocabulary. The tokenizer maps every token to its corresponding ID. This is like looking up the \"meaning\" of the word in BERT's dictionary.\n",
    "\n",
    "- Adds Special Tokens: BERT requires certain special tokens for its tasks, like [CLS] at the beginning of a sentence and [SEP] at the end or between two sentences. The tokenizer adds these in.\n",
    "\n",
    "\n",
    "### Example usage of the tokenizer\n",
    "\n",
    "In the cell below, we see how BERT tokenizes 3 sentences and decodes them back.\n",
    "\n",
    "We'll use the following example sentences:\n",
    "\n",
    "1. \"The sky is blue.\"\n",
    "2. \"Sky is clear today.\"\n",
    "3. \"Look at the clear blue sky.\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/manuelpena/micromamba/envs/cs135_env/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------\n",
      "Examples of tokenizing the sentences with BERT\n",
      "----------------------------------------------\n",
      "The sky is blue. is enocoded as : [101, 1109, 3901, 1110, 2221, 119, 102, 0, 0]\n",
      "Sky is clear today. is enocoded as : [101, 5751, 1110, 2330, 2052, 119, 102, 0, 0]\n",
      "Look at the clear blue sky. is enocoded as : [101, 4785, 1120, 1103, 2330, 2221, 3901, 119, 102]\n",
      "----------------------------------------------\n",
      "Examples of decoding the tokens back to English\n",
      "----------------------------------------------\n",
      "Decoded tokens back into text:  [CLS] The sky is blue. [SEP] [PAD] [PAD]\n",
      "Decoded tokens back into text:  [CLS] Sky is clear today. [SEP] [PAD] [PAD]\n",
      "Decoded tokens back into text:  [CLS] Look at the clear blue sky. [SEP]\n"
     ]
    }
   ],
   "source": [
    "# Import required libraries\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "# # Load pre-trained BERT tokenizer and model\n",
    "sentences = [\"The sky is blue.\", \"Sky is clear today.\", \"Look at the clear blue sky.\"]\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-cased\")\n",
    "encoded_text = tokenizer(sentences, padding=True,\n",
    "                         max_length=10,\n",
    "                         truncation=True)['input_ids']\n",
    "\n",
    "print('----------------------------------------------')\n",
    "print('Examples of tokenizing the sentences with BERT')\n",
    "print('----------------------------------------------')\n",
    "for jj, txt in enumerate(sentences):\n",
    "    print('%s is enocoded as : %s'%(txt, encoded_text[jj]))\n",
    "\n",
    "print('----------------------------------------------')\n",
    "print('Examples of decoding the tokens back to English')\n",
    "print('----------------------------------------------')\n",
    "for enc in encoded_text:\n",
    "    decoded_text = tokenizer.decode(enc)\n",
    "    print(\"Decoded tokens back into text: \", decoded_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Model (Extracting meaningful feature representations from the sentences):\n",
    "\n",
    "Once the text is tokenized and converted into the necessary format, it's fed into the BERT model. \n",
    "The **model** processes these inputs to generate contextual embeddings or representations for each token. These representations can then be utilized for various downstream tasks like classification, entity recognition, and more."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------------------------------------------------------------------------------------------------\n",
      "The sentence \"The sky is blue.\" has been converted to a feature representation of shape (1, 768)\n",
      "-----------------------------------------------------------------------------------------------------------\n",
      "[ 0.229  0.012 -0.139 -0.237 -0.437 -0.553  0.21   0.847  0.17  -0.736\n",
      "  0.019 -0.11   0.38   0.424  0.418  0.06  -0.137  0.701  0.392 -0.189\n",
      "  0.124 -0.139 -0.308  0.088  0.13  -0.334 -0.078 -0.147  0.238  0.005\n",
      " -0.099  0.484 -0.337 -0.415  0.414 -0.065  0.479 -0.037 -0.126  0.332\n",
      " -0.148 -0.099  0.347 -0.211  0.294 -0.679 -2.25  -0.413 -0.084 -0.049\n",
      "  0.376 -0.534  0.055  0.561 -0.002  0.842 -0.419  0.756  0.367  0.318\n",
      "  0.211  0.056  0.023 -0.19   0.364  0.389 -0.157  0.268 -0.03   0.233\n",
      " -0.702 -0.283  0.695 -0.329 -0.286  0.069 -0.524  0.339 -0.061 -0.455\n",
      "  0.245  0.815  0.2    0.419  0.149  0.381 -0.742 -0.676  0.282  0.293\n",
      " -0.382  0.198 -0.349  0.774  0.174 -0.151 -0.058  0.204  0.093  0.529\n",
      "  0.57   0.12   0.581  0.147 -0.09   0.254 -0.341 -0.437  0.545 -2.772\n",
      "  0.31  -0.325  0.084  0.078 -0.234  0.485 -0.077  0.063 -0.329 -0.137\n",
      "  0.027 -0.183  0.016 -0.222 -0.232  0.213  0.134 -0.25   0.276  0.206\n",
      "  0.324  0.676  0.093 -0.66  -0.2    0.16   0.946  0.206 -0.38  -0.176\n",
      " -0.485 -0.195 -2.812  0.265  0.478  0.3   -0.209 -0.104  0.04   0.112\n",
      "  0.329 -0.238  0.05   0.281 -0.185 -0.338 -0.083 -0.117  0.619  0.154\n",
      "  0.118 -0.465  0.412 -0.235 -0.503  0.314  0.722  0.357 -0.163  0.143\n",
      " -0.359  0.315  0.076 -0.128  0.327 -0.435 -0.095  0.385  0.016 -0.088\n",
      " -0.12   0.21   0.311 -0.113  0.431 -0.181  0.526 -0.107 -0.54   0.407\n",
      "  0.129 -0.322 -0.009  0.181 -0.067 -0.281  0.122 -0.826  0.054  0.168\n",
      " -0.023 -0.228 -0.224  0.194 -0.17   4.035  0.176 -0.595  0.095 -0.015\n",
      " -0.259  0.084 -0.242  0.074  0.01  -0.316  0.294  0.225 -0.023  0.142\n",
      "  0.283  0.463 -0.186  0.07  -0.107 -0.103 -0.139  0.606 -0.49  -1.13\n",
      " -0.166 -0.097 -0.541  0.503 -0.502 -0.236 -0.295 -0.448  0.553 -0.066\n",
      "  0.055 -0.065  0.038  0.159 -0.114  0.711  0.436 -0.095  0.479 -0.134\n",
      "  0.646 -0.381 -0.036 -0.421 -0.492  0.259  0.377  0.158 -0.429  0.006\n",
      " -0.18  -0.124  0.73   0.122 -0.675 -0.527 -0.027 -0.025 -0.182 -0.242\n",
      "  0.053 -0.193 -0.107 -3.976 -0.149 -0.066  0.519  0.342 -0.42   0.302\n",
      "  0.683  0.371 -0.788  0.516 -0.064 -0.111  0.182 -0.224  0.375  0.034\n",
      " -0.121 -0.298 -0.132  0.263  0.011 -0.118 -0.018 -0.045 -0.169 -0.348\n",
      " -0.272  0.092 -0.044  0.145 -0.146 -0.059  0.254 -0.009 -1.604  0.567\n",
      " -0.103 -0.069  0.091  0.044  0.603 -0.044 -0.463 -0.004  0.089 -0.328\n",
      " -0.075  0.386  0.405 -0.037  0.208 -0.024  0.279  0.148 -0.14   0.252\n",
      " -0.008 -0.346  0.424  0.644 -0.343 -0.186 -0.322  0.19  -0.012  0.165\n",
      "  0.14  -0.418 -0.439 -0.26   0.539  0.014  0.546  0.111 -0.305  0.764\n",
      "  0.055  0.498  0.861  0.373 -0.246 -0.332  0.417  0.147 -0.082  0.114\n",
      "  0.941 -0.088 -0.193 -0.361  0.381 -0.023  0.169  0.125  0.732 -0.311\n",
      " -0.182  0.239  0.583 -0.866  0.539 -0.488 -0.177  0.353  0.065  0.014\n",
      " -0.662 -1.194 -0.104 -0.039 -0.192  0.098  0.481 -0.189  0.098 -0.332\n",
      " -0.261  0.472 -0.271 -0.219 -0.035 -0.045 -0.408 -0.378 -0.651  0.616\n",
      "  0.359  0.255 -0.178 -0.197  0.212 -0.726 -0.243 -0.192 -0.522 -0.681\n",
      " -0.255  0.348 -0.43  -0.478 -0.175  0.29  -0.267  0.239 -0.453 -0.617\n",
      "  0.607 -0.127  0.773 -0.326  0.596  0.644  0.     0.093  0.356  0.431\n",
      "  0.246  0.113 -0.588 -0.333  0.061 -0.604 -0.636 -0.235 -0.127 -0.258\n",
      " -0.421 -0.419 -0.301 -0.151 -0.294 -0.024  0.285  0.251  0.106 -0.\n",
      " -0.682  0.249 -0.31   0.39   0.094 -0.078 -0.527  0.565  0.107 -0.174\n",
      "  0.042 -0.515  0.079 -0.321  0.161  0.113  0.11  -0.266 -0.102  0.291\n",
      " -1.251  0.024  0.245  0.052  0.562 -0.215 -0.532  0.869 -0.102  0.134\n",
      " -0.21  -0.143 -0.343  0.51  -0.169  0.068 -0.113 -0.481 -0.116 -0.133\n",
      " -0.384  0.237  0.07   0.057 -0.137  0.003 -0.018  0.47  -0.006  0.334\n",
      " -0.311 -0.911 -0.686 -0.467  0.122  0.568  0.386 -0.205  0.692  0.645\n",
      " -0.351  0.64   0.198 -0.371  0.388  0.428 -0.333  0.298 -0.158 -0.496\n",
      " -0.098 -0.081 -0.248  0.027  0.351 -0.273  0.089  0.188 -0.436 -0.251\n",
      "  0.372 -0.298 -0.636  0.084 -0.529 -0.134  0.011 -0.288 -0.195  0.105\n",
      "  0.274 -0.282  0.2    0.417 -0.702  0.136 -0.014  0.372  0.007 -0.28\n",
      "  0.113 -0.269 -0.086  0.157  0.253  0.262 -0.368 -0.352 -0.229 -0.028\n",
      " -0.456 -0.247  0.507  0.057  0.04   0.063  0.324  0.038  0.218 -0.38\n",
      " -0.17   0.828  0.332  0.036 -0.194  0.606  0.797  0.037 -0.407 -0.31\n",
      " -0.334 -0.307 -0.274 -0.05   0.024 -0.458 -0.015 -0.598  2.285  0.564\n",
      "  0.089 -0.711  0.112 -0.069 -0.316  0.503 -0.647  0.168 -0.109 -0.208\n",
      "  0.058  0.313  0.474  0.093 -0.189  0.014 -0.607 -0.102 -0.327  0.843\n",
      "  0.108 -0.3    0.224  0.3   -0.048  0.047 -0.083  0.678 -0.197  0.344\n",
      "  0.093  0.446 -0.48   0.555 -0.111 -0.241 -0.22   0.43  -0.073 -0.706\n",
      "  0.553  0.015 -0.054  0.844 -0.276 -0.004  0.76   0.241 -0.276 -0.174\n",
      " -0.62   0.4   -0.481 -0.066  0.196  0.126 -0.322  0.182  0.088  0.286\n",
      "  0.508 -0.098 -0.105 -0.292 -0.307 -0.048  0.379 -0.52  -0.148  0.47\n",
      "  0.713  0.038  0.383 -0.273  0.396 -0.052 -0.174 -3.231  0.072  0.401\n",
      "  0.072  0.157  0.314  0.253 -0.064  0.02  -0.505  0.275  0.279  0.55\n",
      "  0.405  0.329  0.481  0.36  -0.42  -0.327 -0.027  0.326  0.06   0.256\n",
      " -0.386 -0.468  0.453  0.071 -0.214  0.117  0.235  0.139  0.553  0.182\n",
      "  0.096 -0.121 -0.138 -0.385 -0.053  0.431 -0.02  -0.153  0.448 -0.011\n",
      " -0.19  -0.02  -0.071  0.283 -0.392  0.62  -0.415 -0.095  0.15   0.102\n",
      "  0.116  0.293 -0.159 -0.092  0.104 -0.019 -0.164  0.012  0.409  0.218\n",
      "  0.066  0.476 -0.126 -0.389  0.14   0.161 -0.176 -0.16  -0.273  0.431\n",
      " -0.111 -0.105  0.166  0.799  0.27   0.291  0.019 -0.159  0.156 -0.682\n",
      "  0.129 -0.086 -8.11   0.07  -0.225 -0.443  0.198 -0.219 -0.026 -0.062\n",
      "  0.082  0.007  0.118  0.542  0.154 -0.487  0.419  0.43 ]\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import BertTokenizer, BertModel\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "# Initialize BERT tokenizer and model\n",
    "model_name = \"bert-base-uncased\"\n",
    "tokenizer = BertTokenizer.from_pretrained(model_name)\n",
    "model = BertModel.from_pretrained(model_name)\n",
    "\n",
    "def get_bert_embedding(sentence_list, pooling_strategy='cls'):\n",
    "    embedding_list = []\n",
    "    for nn, sentence in enumerate(sentence_list):\n",
    "        if (nn%100==0)&(nn>0):\n",
    "            print('Done with %d sentences'%nn)\n",
    "        \n",
    "        # Tokenize the sentence and get the output from BERT\n",
    "        inputs = tokenizer(sentence, return_tensors=\"pt\")\n",
    "        with torch.no_grad():\n",
    "            outputs = model(**inputs)\n",
    "        # Take the embeddings from the last hidden state (optionally, one can use pooling techniques for different representations)\n",
    "        # Here, we take the [CLS] token representation as the sentence embedding\n",
    "        last_hidden_states = outputs.last_hidden_state[0]\n",
    "        \n",
    "        # Pooling strategies\n",
    "        if pooling_strategy == \"cls\":\n",
    "            sentence_embedding = last_hidden_states[0]\n",
    "        elif pooling_strategy == \"mean\":\n",
    "            sentence_embedding = torch.mean(last_hidden_states, dim=0)\n",
    "        elif pooling_strategy == \"max\":\n",
    "            sentence_embedding, _ = torch.max(last_hidden_states, dim=0)\n",
    "        else:\n",
    "            raise ValueError(f\"Unknown pooling strategy: {pooling_strategy}\")\n",
    "        \n",
    "        embedding_list.append(sentence_embedding)\n",
    "    return torch.stack(embedding_list)\n",
    "\n",
    "sentence = [sentences[0]]\n",
    "embedding = get_bert_embedding(sentence)\n",
    "\n",
    "np.set_printoptions(precision=3, suppress=True)\n",
    "print('-----------------------------------------------------------------------------------------------------------')\n",
    "print('The sentence \"%s\" has been converted to a feature representation of shape %s'%(sentence[0], embedding.numpy().shape))\n",
    "print('-----------------------------------------------------------------------------------------------------------')\n",
    "print(embedding.numpy()[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data...\n"
     ]
    }
   ],
   "source": [
    "print('Loading data...')\n",
    "x_train_df = pd.read_csv('../data_reviews/x_train.csv')\n",
    "x_test_df = pd.read_csv('../data_reviews/x_test.csv')\n",
    "\n",
    "tr_text_list = x_train_df['text'].values.tolist()\n",
    "te_text_list = x_test_df['text'].values.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating embeddings for train sequences...\n",
      "Done with 100 sentences\n",
      "Done with 200 sentences\n",
      "Done with 300 sentences\n",
      "Done with 400 sentences\n",
      "Done with 500 sentences\n",
      "Done with 600 sentences\n",
      "Done with 700 sentences\n",
      "Done with 800 sentences\n",
      "Done with 900 sentences\n",
      "Done with 1000 sentences\n",
      "Done with 1100 sentences\n",
      "Done with 1200 sentences\n",
      "Done with 1300 sentences\n",
      "Done with 1400 sentences\n",
      "Done with 1500 sentences\n",
      "Done with 1600 sentences\n",
      "Done with 1700 sentences\n",
      "Done with 1800 sentences\n",
      "Done with 1900 sentences\n",
      "Done with 2000 sentences\n",
      "Done with 2100 sentences\n",
      "Done with 2200 sentences\n",
      "Done with 2300 sentences\n",
      "Generating embeddings for test sequences...\n",
      "Done with 100 sentences\n",
      "Done with 200 sentences\n",
      "Done with 300 sentences\n",
      "Done with 400 sentences\n",
      "Done with 500 sentences\n"
     ]
    }
   ],
   "source": [
    "print('Generating embeddings for train sequences...')\n",
    "tr_embedding = get_bert_embedding(tr_text_list)\n",
    "\n",
    "print('Generating embeddings for test sequences...')\n",
    "te_embedding = get_bert_embedding(te_text_list)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving the train and test embeddings to /Users/manuelpena/Documents/Tufts/fifth_semester/cs135-24f-assignments/projectA/data_reviews\n"
     ]
    }
   ],
   "source": [
    "tr_embeddings_ND = tr_embedding.numpy()\n",
    "te_embeddings_ND = te_embedding.numpy()\n",
    "\n",
    "save_dir = os.path.abspath('../data_reviews/')\n",
    "print('Saving the train and test embeddings to %s'%save_dir)\n",
    "\n",
    "np.save(os.path.join(save_dir, 'x_train_BERT_embeddings.npy'), tr_embeddings_ND)\n",
    "np.save(os.path.join(save_dir, 'x_test_BERT_embeddings.npy'), te_embeddings_ND)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import classification_report, confusion_matrix, roc_auc_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the precomputed BERT embeddings\n",
    "train_embeddings = np.load(os.path.join(save_dir, 'x_train_BERT_embeddings.npy'))\n",
    "test_embeddings = np.load(os.path.join(save_dir, 'x_test_BERT_embeddings.npy'))\n",
    "\n",
    "# Load the labels (we only have y_train, so we'll still fit on it)\n",
    "y_train_df = pd.read_csv('../data_reviews/y_train.csv')\n",
    "y_train = y_train_df['is_positive_sentiment'].values\n",
    "\n",
    "# Standardize the embeddings before training SVC\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(train_embeddings)\n",
    "X_test_scaled = scaler.transform(test_embeddings)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Show similarity between reviews using the embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-2 {color: black;background-color: white;}#sk-container-id-2 pre{padding: 0;}#sk-container-id-2 div.sk-toggleable {background-color: white;}#sk-container-id-2 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-2 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-2 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-2 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-2 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-2 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-2 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-2 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-2 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-2 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-2 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-2 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-2 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-2 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-2 div.sk-item {position: relative;z-index: 1;}#sk-container-id-2 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-2 div.sk-item::before, #sk-container-id-2 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-2 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-2 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-2 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-2 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-2 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-2 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-2 div.sk-label-container {text-align: center;}#sk-container-id-2 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-2 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-2\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>GridSearchCV(cv=5, estimator=SVC(class_weight=&#x27;balanced&#x27;, probability=True),\n",
       "             param_grid={&#x27;C&#x27;: [0.1, 1, 10, 100], &#x27;gamma&#x27;: [&#x27;scale&#x27;, &#x27;auto&#x27;],\n",
       "                         &#x27;kernel&#x27;: [&#x27;linear&#x27;, &#x27;rbf&#x27;]},\n",
       "             scoring=&#x27;roc_auc&#x27;)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item sk-dashed-wrapped\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-4\" type=\"checkbox\" ><label for=\"sk-estimator-id-4\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">GridSearchCV</label><div class=\"sk-toggleable__content\"><pre>GridSearchCV(cv=5, estimator=SVC(class_weight=&#x27;balanced&#x27;, probability=True),\n",
       "             param_grid={&#x27;C&#x27;: [0.1, 1, 10, 100], &#x27;gamma&#x27;: [&#x27;scale&#x27;, &#x27;auto&#x27;],\n",
       "                         &#x27;kernel&#x27;: [&#x27;linear&#x27;, &#x27;rbf&#x27;]},\n",
       "             scoring=&#x27;roc_auc&#x27;)</pre></div></div></div><div class=\"sk-parallel\"><div class=\"sk-parallel-item\"><div class=\"sk-item\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-5\" type=\"checkbox\" ><label for=\"sk-estimator-id-5\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">estimator: SVC</label><div class=\"sk-toggleable__content\"><pre>SVC(class_weight=&#x27;balanced&#x27;, probability=True)</pre></div></div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-6\" type=\"checkbox\" ><label for=\"sk-estimator-id-6\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">SVC</label><div class=\"sk-toggleable__content\"><pre>SVC(class_weight=&#x27;balanced&#x27;, probability=True)</pre></div></div></div></div></div></div></div></div></div></div>"
      ],
      "text/plain": [
       "GridSearchCV(cv=5, estimator=SVC(class_weight='balanced', probability=True),\n",
       "             param_grid={'C': [0.1, 1, 10, 100], 'gamma': ['scale', 'auto'],\n",
       "                         'kernel': ['linear', 'rbf']},\n",
       "             scoring='roc_auc')"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Train SVC (you can tune hyperparameters via GridSearchCV later if needed) 9min\n",
    "param_grid = {\n",
    "    'C': [0.1, 1, 10, 100],\n",
    "    'gamma': ['scale', 'auto'],\n",
    "    'kernel': ['linear', 'rbf']\n",
    "}\n",
    "\n",
    "# 16 min\n",
    "# param_grid = {\n",
    "#     'C': [0.1, 1],        \n",
    "#     'gamma': ['scale', 'auto'],     \n",
    "#     'kernel': ['linear', 'rbf', 'poly'],\n",
    "#     'degree': [2, 3],           \n",
    "# }\n",
    "# Best parameters found:  {'C': 1, 'degree': 2, 'gamma': 'auto', 'kernel': 'rbf'}\n",
    "# Best cross-validation AUC:  0.96721875\n",
    "# Training AUC: 0.9967\n",
    "# Training AUC: 0.9967\n",
    "# Predicted probabilities saved to /Users/manuelpena/Documents/Tufts/fifth_semester/cs135-24f-assignments/projectA/data_reviews/yproba1_test.txt\n",
    "\n",
    "\n",
    "# Initialize SVC\n",
    "svc = SVC(class_weight='balanced', probability=True)\n",
    "\n",
    "# Initialize GridSearchCV\n",
    "grid_search = GridSearchCV(\n",
    "    estimator=svc, \n",
    "    param_grid=param_grid, \n",
    "    scoring='roc_auc', \n",
    "    cv=5\n",
    ")\n",
    "\n",
    "\n",
    "# Fit GridSearchCV\n",
    "grid_search.fit(X_train_scaled, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameters found:  {'C': 1, 'gamma': 'auto', 'kernel': 'rbf'}\n",
      "Best cross-validation AUC:  0.96721875\n"
     ]
    }
   ],
   "source": [
    "# Retrieve the best parameters and best score\n",
    "print(\"Best parameters found: \", grid_search.best_params_)\n",
    "print(\"Best cross-validation AUC: \", grid_search.best_score_)\n",
    "\n",
    "# # Use the best estimator from GridSearchCV to make predictions\n",
    "best_svc = grid_search.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training AUC: 0.9967\n",
      "Training AUC: 0.9967\n"
     ]
    }
   ],
   "source": [
    "# Make predictions (probabilities) for training set\n",
    "y_pred_prob_train = best_svc.predict_proba(X_train_scaled)[:, 1]\n",
    "auc_train = roc_auc_score(y_train, y_pred_prob_train)\n",
    "print(f'Training AUC: {auc_train:.4f}')\n",
    "\n",
    "\n",
    "# Calculate AUC for the training set\n",
    "auc_train = roc_auc_score(y_train, y_pred_prob_train)\n",
    "print(f'Training AUC: {auc_train:.4f}')\n",
    "\n",
    "# Make predictions on test set\n",
    "y_pred_prob_test = best_svc.predict_proba(X_test_scaled)[:, 1] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted probabilities saved to /Users/manuelpena/Documents/Tufts/fifth_semester/cs135-24f-assignments/projectA/data_reviews/yproba1_test.txt\n"
     ]
    }
   ],
   "source": [
    "# Save predictions to a text file (y_proba1_test.txt)\n",
    "output_file = os.path.join(save_dir, 'yproba1_test.txt')\n",
    "np.savetxt(output_file, y_pred_prob_test, fmt='%.6f')\n",
    "\n",
    "print(f'Predicted probabilities saved to {output_file}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
