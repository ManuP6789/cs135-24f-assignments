{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generating Embeddings with BERT (Bidirectional Encoder Representations from Transformers) \n",
    "\n",
    "### What is BERT?\n",
    "BERT is a large state-of-the-art neural network that has been trained on a large corpora of text (millions of sentences). Its applications include but are not limited to:\n",
    "\n",
    "- Sentiment analysis\n",
    "- Text classification\n",
    "- Question answering systems\n",
    " \n",
    "In this notebook, we walk through how BERT generates fixed-length embeddings (features) from a sentence. You could think of these embeddings as an alternate feature extraction technique compared to bag of words. The BERT model has 2 main components as shown below \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting transformers\n",
      "  Downloading transformers-4.45.2-py3-none-any.whl.metadata (44 kB)\n",
      "Collecting filelock (from transformers)\n",
      "  Downloading filelock-3.16.1-py3-none-any.whl.metadata (2.9 kB)\n",
      "Collecting huggingface-hub<1.0,>=0.23.2 (from transformers)\n",
      "  Downloading huggingface_hub-0.25.2-py3-none-any.whl.metadata (13 kB)\n",
      "Requirement already satisfied: numpy>=1.17 in /Users/manuelpena/micromamba/envs/cs135_env/lib/python3.10/site-packages (from transformers) (1.23.5)\n",
      "Requirement already satisfied: packaging>=20.0 in /Users/manuelpena/micromamba/envs/cs135_env/lib/python3.10/site-packages (from transformers) (24.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /Users/manuelpena/micromamba/envs/cs135_env/lib/python3.10/site-packages (from transformers) (6.0.1)\n",
      "Collecting regex!=2019.12.17 (from transformers)\n",
      "  Downloading regex-2024.9.11-cp310-cp310-macosx_11_0_arm64.whl.metadata (40 kB)\n",
      "Requirement already satisfied: requests in /Users/manuelpena/micromamba/envs/cs135_env/lib/python3.10/site-packages (from transformers) (2.32.3)\n",
      "Collecting safetensors>=0.4.1 (from transformers)\n",
      "  Downloading safetensors-0.4.5-cp310-cp310-macosx_11_0_arm64.whl.metadata (3.8 kB)\n",
      "Collecting tokenizers<0.21,>=0.20 (from transformers)\n",
      "  Downloading tokenizers-0.20.1-cp310-cp310-macosx_11_0_arm64.whl.metadata (6.7 kB)\n",
      "Collecting tqdm>=4.27 (from transformers)\n",
      "  Downloading tqdm-4.66.5-py3-none-any.whl.metadata (57 kB)\n",
      "Collecting fsspec>=2023.5.0 (from huggingface-hub<1.0,>=0.23.2->transformers)\n",
      "  Downloading fsspec-2024.9.0-py3-none-any.whl.metadata (11 kB)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /Users/manuelpena/micromamba/envs/cs135_env/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.23.2->transformers) (4.11.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Users/manuelpena/micromamba/envs/cs135_env/lib/python3.10/site-packages (from requests->transformers) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/manuelpena/micromamba/envs/cs135_env/lib/python3.10/site-packages (from requests->transformers) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/manuelpena/micromamba/envs/cs135_env/lib/python3.10/site-packages (from requests->transformers) (2.2.2)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/manuelpena/micromamba/envs/cs135_env/lib/python3.10/site-packages (from requests->transformers) (2024.7.4)\n",
      "Downloading transformers-4.45.2-py3-none-any.whl (9.9 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m9.9/9.9 MB\u001b[0m \u001b[31m7.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading huggingface_hub-0.25.2-py3-none-any.whl (436 kB)\n",
      "Downloading regex-2024.9.11-cp310-cp310-macosx_11_0_arm64.whl (284 kB)\n",
      "Downloading safetensors-0.4.5-cp310-cp310-macosx_11_0_arm64.whl (381 kB)\n",
      "Downloading tokenizers-0.20.1-cp310-cp310-macosx_11_0_arm64.whl (2.6 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.6/2.6 MB\u001b[0m \u001b[31m7.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading tqdm-4.66.5-py3-none-any.whl (78 kB)\n",
      "Downloading filelock-3.16.1-py3-none-any.whl (16 kB)\n",
      "Downloading fsspec-2024.9.0-py3-none-any.whl (179 kB)\n",
      "Installing collected packages: tqdm, safetensors, regex, fsspec, filelock, huggingface-hub, tokenizers, transformers\n",
      "Successfully installed filelock-3.16.1 fsspec-2024.9.0 huggingface-hub-0.25.2 regex-2024.9.11 safetensors-0.4.5 tokenizers-0.20.1 tqdm-4.66.5 transformers-4.45.2\n"
     ]
    }
   ],
   "source": [
    "# Install the required libraries (if not already installed)\n",
    "!pip install transformers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Tokenizer (Converting sentences into series of numerical tokens):\n",
    "\n",
    "The tokenizer in BERT is like a translator that converts sentences into a series of numerical tokens that the BERT model can understand. Specifically, it does the following:\n",
    "\n",
    "- Splits Text: It breaks down sentences into smaller pieces called tokens. These tokens can be as short as one character or as long as one word. For example, the word \"chatting\" might be split into \"chat\" and \"##ting\".\n",
    "\n",
    "- Converts Tokens to IDs: Each token has a unique ID in BERT's vocabulary. The tokenizer maps every token to its corresponding ID. This is like looking up the \"meaning\" of the word in BERT's dictionary.\n",
    "\n",
    "- Adds Special Tokens: BERT requires certain special tokens for its tasks, like [CLS] at the beginning of a sentence and [SEP] at the end or between two sentences. The tokenizer adds these in.\n",
    "\n",
    "\n",
    "### Example usage of the tokenizer\n",
    "\n",
    "In the cell below, we see how BERT tokenizes 3 sentences and decodes them back.\n",
    "\n",
    "We'll use the following example sentences:\n",
    "\n",
    "1. \"The sky is blue.\"\n",
    "2. \"Sky is clear today.\"\n",
    "3. \"Look at the clear blue sky.\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/manuelpena/micromamba/envs/cs135_env/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "None of PyTorch, TensorFlow >= 2.0, or Flax have been found. Models won't be available and only tokenizers, configuration and file/data utilities can be used.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------\n",
      "Examples of tokenizing the sentences with BERT\n",
      "----------------------------------------------\n",
      "The sky is blue. is enocoded as : [101, 1109, 3901, 1110, 2221, 119, 102, 0, 0]\n",
      "Sky is clear today. is enocoded as : [101, 5751, 1110, 2330, 2052, 119, 102, 0, 0]\n",
      "Look at the clear blue sky. is enocoded as : [101, 4785, 1120, 1103, 2330, 2221, 3901, 119, 102]\n",
      "----------------------------------------------\n",
      "Examples of decoding the tokens back to English\n",
      "----------------------------------------------\n",
      "Decoded tokens back into text:  [CLS] The sky is blue. [SEP] [PAD] [PAD]\n",
      "Decoded tokens back into text:  [CLS] Sky is clear today. [SEP] [PAD] [PAD]\n",
      "Decoded tokens back into text:  [CLS] Look at the clear blue sky. [SEP]\n"
     ]
    }
   ],
   "source": [
    "# Import required libraries\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "# # Load pre-trained BERT tokenizer and model\n",
    "sentences = [\"The sky is blue.\", \"Sky is clear today.\", \"Look at the clear blue sky.\"]\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-cased\")\n",
    "encoded_text = tokenizer(sentences, padding=True,\n",
    "                         max_length=10,\n",
    "                         truncation=True)['input_ids']\n",
    "\n",
    "print('----------------------------------------------')\n",
    "print('Examples of tokenizing the sentences with BERT')\n",
    "print('----------------------------------------------')\n",
    "for jj, txt in enumerate(sentences):\n",
    "    print('%s is enocoded as : %s'%(txt, encoded_text[jj]))\n",
    "\n",
    "print('----------------------------------------------')\n",
    "print('Examples of decoding the tokens back to English')\n",
    "print('----------------------------------------------')\n",
    "for enc in encoded_text:\n",
    "    decoded_text = tokenizer.decode(enc)\n",
    "    print(\"Decoded tokens back into text: \", decoded_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Model (Extracting meaningful feature representations from the sentences):\n",
    "\n",
    "Once the text is tokenized and converted into the necessary format, it's fed into the BERT model. \n",
    "The **model** processes these inputs to generate contextual embeddings or representations for each token. These representations can then be utilized for various downstream tasks like classification, entity recognition, and more."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pytorch\n",
      "  Downloading pytorch-1.0.2.tar.gz (689 bytes)\n",
      "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hRequirement already satisfied: torchvision in /Users/manuelpena/micromamba/envs/cs135_env/lib/python3.10/site-packages (0.19.1)\n",
      "Requirement already satisfied: torchaudio in /Users/manuelpena/micromamba/envs/cs135_env/lib/python3.10/site-packages (2.4.1)\n",
      "Requirement already satisfied: numpy in /Users/manuelpena/micromamba/envs/cs135_env/lib/python3.10/site-packages (from torchvision) (1.23.5)\n",
      "Requirement already satisfied: torch==2.4.1 in /Users/manuelpena/micromamba/envs/cs135_env/lib/python3.10/site-packages (from torchvision) (2.4.1)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /Users/manuelpena/micromamba/envs/cs135_env/lib/python3.10/site-packages (from torchvision) (10.4.0)\n",
      "Requirement already satisfied: filelock in /Users/manuelpena/micromamba/envs/cs135_env/lib/python3.10/site-packages (from torch==2.4.1->torchvision) (3.16.1)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in /Users/manuelpena/micromamba/envs/cs135_env/lib/python3.10/site-packages (from torch==2.4.1->torchvision) (4.11.0)\n",
      "Requirement already satisfied: sympy in /Users/manuelpena/micromamba/envs/cs135_env/lib/python3.10/site-packages (from torch==2.4.1->torchvision) (1.13.3)\n",
      "Requirement already satisfied: networkx in /Users/manuelpena/micromamba/envs/cs135_env/lib/python3.10/site-packages (from torch==2.4.1->torchvision) (3.4.1)\n",
      "Requirement already satisfied: jinja2 in /Users/manuelpena/micromamba/envs/cs135_env/lib/python3.10/site-packages (from torch==2.4.1->torchvision) (3.1.4)\n",
      "Requirement already satisfied: fsspec in /Users/manuelpena/micromamba/envs/cs135_env/lib/python3.10/site-packages (from torch==2.4.1->torchvision) (2024.9.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /Users/manuelpena/micromamba/envs/cs135_env/lib/python3.10/site-packages (from jinja2->torch==2.4.1->torchvision) (2.1.3)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /Users/manuelpena/micromamba/envs/cs135_env/lib/python3.10/site-packages (from sympy->torch==2.4.1->torchvision) (1.3.0)\n",
      "Building wheels for collected packages: pytorch\n",
      "  Building wheel for pytorch (setup.py) ... \u001b[?25lerror\n",
      "  \u001b[1;31merror\u001b[0m: \u001b[1msubprocess-exited-with-error\u001b[0m\n",
      "  \n",
      "  \u001b[31m×\u001b[0m \u001b[32mpython setup.py bdist_wheel\u001b[0m did not run successfully.\n",
      "  \u001b[31m│\u001b[0m exit code: \u001b[1;36m1\u001b[0m\n",
      "  \u001b[31m╰─>\u001b[0m \u001b[31m[6 lines of output]\u001b[0m\n",
      "  \u001b[31m   \u001b[0m Traceback (most recent call last):\n",
      "  \u001b[31m   \u001b[0m   File \"<string>\", line 2, in <module>\n",
      "  \u001b[31m   \u001b[0m   File \"<pip-setuptools-caller>\", line 34, in <module>\n",
      "  \u001b[31m   \u001b[0m   File \"/private/var/folders/5p/dl65mhts5vl1k_k3rt3lrt6c0000gn/T/pip-install-1y07zc88/pytorch_600d8a8751ab429fb076746da265fc99/setup.py\", line 15, in <module>\n",
      "  \u001b[31m   \u001b[0m     raise Exception(message)\n",
      "  \u001b[31m   \u001b[0m Exception: You tried to install \"pytorch\". The package named for PyTorch is \"torch\"\n",
      "  \u001b[31m   \u001b[0m \u001b[31m[end of output]\u001b[0m\n",
      "  \n",
      "  \u001b[1;35mnote\u001b[0m: This error originates from a subprocess, and is likely not a problem with pip.\n",
      "\u001b[31m  ERROR: Failed building wheel for pytorch\u001b[0m\u001b[31m\n",
      "\u001b[0m\u001b[?25h  Running setup.py clean for pytorch\n",
      "Failed to build pytorch\n",
      "\u001b[31mERROR: ERROR: Failed to build installable wheels for some pyproject.toml based projects (pytorch)\u001b[0m\u001b[31m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip install pytorch torchvision torchaudio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting torch\n",
      "  Downloading torch-2.4.1-cp310-none-macosx_11_0_arm64.whl.metadata (26 kB)\n",
      "Requirement already satisfied: filelock in /Users/manuelpena/micromamba/envs/cs135_env/lib/python3.10/site-packages (from torch) (3.16.1)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in /Users/manuelpena/micromamba/envs/cs135_env/lib/python3.10/site-packages (from torch) (4.11.0)\n",
      "Collecting sympy (from torch)\n",
      "  Downloading sympy-1.13.3-py3-none-any.whl.metadata (12 kB)\n",
      "Collecting networkx (from torch)\n",
      "  Downloading networkx-3.4.1-py3-none-any.whl.metadata (6.3 kB)\n",
      "Requirement already satisfied: jinja2 in /Users/manuelpena/micromamba/envs/cs135_env/lib/python3.10/site-packages (from torch) (3.1.4)\n",
      "Requirement already satisfied: fsspec in /Users/manuelpena/micromamba/envs/cs135_env/lib/python3.10/site-packages (from torch) (2024.9.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /Users/manuelpena/micromamba/envs/cs135_env/lib/python3.10/site-packages (from jinja2->torch) (2.1.3)\n",
      "Collecting mpmath<1.4,>=1.1.0 (from sympy->torch)\n",
      "  Using cached mpmath-1.3.0-py3-none-any.whl.metadata (8.6 kB)\n",
      "Downloading torch-2.4.1-cp310-none-macosx_11_0_arm64.whl (62.1 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.1/62.1 MB\u001b[0m \u001b[31m7.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading networkx-3.4.1-py3-none-any.whl (1.7 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m7.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading sympy-1.13.3-py3-none-any.whl (6.2 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.2/6.2 MB\u001b[0m \u001b[31m8.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hUsing cached mpmath-1.3.0-py3-none-any.whl (536 kB)\n",
      "Installing collected packages: mpmath, sympy, networkx, torch\n",
      "Successfully installed mpmath-1.3.0 networkx-3.4.1 sympy-1.13.3 torch-2.4.1\n"
     ]
    }
   ],
   "source": [
    "!pip install torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "\nBertModel requires the PyTorch library but it was not found in your environment. Checkout the instructions on the\ninstallation page: https://pytorch.org/get-started/locally/ and follow the ones that match your environment.\nPlease note that you may need to restart your runtime after installation.\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[23], line 9\u001b[0m\n\u001b[1;32m      7\u001b[0m model_name \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbert-base-uncased\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m      8\u001b[0m tokenizer \u001b[38;5;241m=\u001b[39m BertTokenizer\u001b[38;5;241m.\u001b[39mfrom_pretrained(model_name)\n\u001b[0;32m----> 9\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mBertModel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_pretrained\u001b[49m(model_name)\n\u001b[1;32m     11\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget_bert_embedding\u001b[39m(sentence_list, pooling_strategy\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcls\u001b[39m\u001b[38;5;124m'\u001b[39m):\n\u001b[1;32m     12\u001b[0m     embedding_list \u001b[38;5;241m=\u001b[39m []\n",
      "File \u001b[0;32m~/micromamba/envs/cs135_env/lib/python3.10/site-packages/transformers/utils/import_utils.py:1637\u001b[0m, in \u001b[0;36mDummyObject.__getattribute__\u001b[0;34m(cls, key)\u001b[0m\n\u001b[1;32m   1635\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m key\u001b[38;5;241m.\u001b[39mstartswith(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m key \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_from_config\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m   1636\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__getattribute__\u001b[39m(key)\n\u001b[0;32m-> 1637\u001b[0m \u001b[43mrequires_backends\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mcls\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mcls\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_backends\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/micromamba/envs/cs135_env/lib/python3.10/site-packages/transformers/utils/import_utils.py:1625\u001b[0m, in \u001b[0;36mrequires_backends\u001b[0;34m(obj, backends)\u001b[0m\n\u001b[1;32m   1623\u001b[0m failed \u001b[38;5;241m=\u001b[39m [msg\u001b[38;5;241m.\u001b[39mformat(name) \u001b[38;5;28;01mfor\u001b[39;00m available, msg \u001b[38;5;129;01min\u001b[39;00m checks \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m available()]\n\u001b[1;32m   1624\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m failed:\n\u001b[0;32m-> 1625\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(failed))\n",
      "\u001b[0;31mImportError\u001b[0m: \nBertModel requires the PyTorch library but it was not found in your environment. Checkout the instructions on the\ninstallation page: https://pytorch.org/get-started/locally/ and follow the ones that match your environment.\nPlease note that you may need to restart your runtime after installation.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import BertTokenizer, BertModel\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "# Initialize BERT tokenizer and model\n",
    "model_name = \"bert-base-uncased\"\n",
    "tokenizer = BertTokenizer.from_pretrained(model_name)\n",
    "model = BertModel.from_pretrained(model_name)\n",
    "\n",
    "def get_bert_embedding(sentence_list, pooling_strategy='cls'):\n",
    "    embedding_list = []\n",
    "    for nn, sentence in enumerate(sentence_list):\n",
    "        if (nn%100==0)&(nn>0):\n",
    "            print('Done with %d sentences'%nn)\n",
    "        \n",
    "        # Tokenize the sentence and get the output from BERT\n",
    "        inputs = tokenizer(sentence, return_tensors=\"pt\")\n",
    "        with torch.no_grad():\n",
    "            outputs = model(**inputs)\n",
    "        # Take the embeddings from the last hidden state (optionally, one can use pooling techniques for different representations)\n",
    "        # Here, we take the [CLS] token representation as the sentence embedding\n",
    "        last_hidden_states = outputs.last_hidden_state[0]\n",
    "        \n",
    "        # Pooling strategies\n",
    "        if pooling_strategy == \"cls\":\n",
    "            sentence_embedding = last_hidden_states[0]\n",
    "        elif pooling_strategy == \"mean\":\n",
    "            sentence_embedding = torch.mean(last_hidden_states, dim=0)\n",
    "        elif pooling_strategy == \"max\":\n",
    "            sentence_embedding, _ = torch.max(last_hidden_states, dim=0)\n",
    "        else:\n",
    "            raise ValueError(f\"Unknown pooling strategy: {pooling_strategy}\")\n",
    "        \n",
    "        embedding_list.append(sentence_embedding)\n",
    "    return torch.stack(embedding_list)\n",
    "\n",
    "sentence = [sentences[0]]\n",
    "embedding = get_bert_embedding(sentence)\n",
    "\n",
    "np.set_printoptions(precision=3, suppress=True)\n",
    "print('-----------------------------------------------------------------------------------------------------------')\n",
    "print('The sentence \"%s\" has been converted to a feature representation of shape %s'%(sentence[0], embedding.numpy().shape))\n",
    "print('-----------------------------------------------------------------------------------------------------------')\n",
    "print(embedding.numpy()[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate embeddings from BERT for the movie review train and test sets\n",
    "\n",
    "Below, we generate the BERT embeddings for the movie reviews dataset provided to you. The embeddings are stored as numpy files in the data_reviews folder:\n",
    "\n",
    "- x_train_BERT_embeddings.npy : matrix of size 2400 x 768 containing 768 length features for each of the 2400 sentences in the training set\n",
    "- x_test_BERT_embeddings.npy : matrix of size 600 x 768 containing 768 length features for each of the 600 sentences in the test set\n",
    "\n",
    "Please note that these embeddings are **already provided to you in the data_reviews folder**. You can directly use the provided feature embeddings as inputs to your choice of classifier. If you would like to generate these feature representations again, run the code cells below. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data...\n"
     ]
    }
   ],
   "source": [
    "print('Loading data...')\n",
    "x_train_df = pd.read_csv('../data_reviews/x_train.csv')\n",
    "x_test_df = pd.read_csv('../data_reviews/x_test.csv')\n",
    "\n",
    "tr_text_list = x_train_df['text'].values.tolist()\n",
    "te_text_list = x_test_df['text'].values.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating embeddings for train sequences...\n",
      "Done with 0 sentences\n",
      "Done with 100 sentences\n",
      "Done with 200 sentences\n",
      "Done with 300 sentences\n",
      "Done with 400 sentences\n",
      "Done with 500 sentences\n",
      "Done with 600 sentences\n",
      "Done with 700 sentences\n",
      "Done with 800 sentences\n",
      "Done with 900 sentences\n",
      "Done with 1000 sentences\n",
      "Done with 1100 sentences\n",
      "Done with 1200 sentences\n",
      "Done with 1300 sentences\n",
      "Done with 1400 sentences\n",
      "Done with 1500 sentences\n",
      "Done with 1600 sentences\n",
      "Done with 1700 sentences\n",
      "Done with 1800 sentences\n",
      "Done with 1900 sentences\n",
      "Done with 2000 sentences\n",
      "Done with 2100 sentences\n",
      "Done with 2200 sentences\n",
      "Done with 2300 sentences\n",
      "Generating embeddings for test sequences...\n",
      "Done with 0 sentences\n",
      "Done with 100 sentences\n",
      "Done with 200 sentences\n",
      "Done with 300 sentences\n",
      "Done with 400 sentences\n",
      "Done with 500 sentences\n"
     ]
    }
   ],
   "source": [
    "print('Generating embeddings for train sequences...')\n",
    "tr_embedding = get_bert_embedding(tr_text_list)\n",
    "\n",
    "print('Generating embeddings for test sequences...')\n",
    "te_embedding = get_bert_embedding(te_text_list)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving the train and test embeddings to /cluster/tufts/hugheslab/prath01/projects/cs135-23f-staffonly/proj_src/projA/data_reviews\n"
     ]
    }
   ],
   "source": [
    "tr_embeddings_ND = tr_embedding.numpy()\n",
    "te_embeddings_ND = te_embedding.numpy()\n",
    "\n",
    "save_dir = os.path.abspath('../data_reviews/')\n",
    "print('Saving the train and test embeddings to %s'%save_dir)\n",
    "\n",
    "np.save(os.path.join(save_dir, 'x_train_BERT_embeddings.npy'), tr_embeddings_ND)\n",
    "np.save(os.path.join(save_dir, 'x_test_BERT_embeddings.npy'), te_embeddings_ND)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Show similarity between reviews using the embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import calc_k_nearest_neighbors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------------------------------------------------------------\n",
      "Reviews that resemble to the sentence : \n",
      "Worst customer service.\n",
      "------------------------------------------------------------------------------------\n",
      "0) Worst customer service.\n",
      "1) Poor product.\n",
      "2) poor quality and service.\n",
      "3) Bad Quality.\n",
      "4) Bad Reception.\n",
      "------------------------------------------------------------------------------------\n",
      "Reviews that resemble to the sentence : \n",
      "I'm very disappointed with my decision.\n",
      "------------------------------------------------------------------------------------\n",
      "0) I'm very disappointed with my decision.\n",
      "1) I'm a bit disappointed.\n",
      "2) I was very disappointed in the movie.  \n",
      "3) I'm still trying to get over how bad it was.  \n",
      "4) I can't wait to go back.\n",
      "------------------------------------------------------------------------------------\n",
      "Reviews that resemble to the sentence : \n",
      "There's a horrible tick sound in the background on all my calls that I have never experienced before.\n",
      "------------------------------------------------------------------------------------\n",
      "0) There's a horrible tick sound in the background on all my calls that I have never experienced before.\n",
      "1) The only thing that I think could improve is the sound leaks out from the headset.\n",
      "2) I'm really disappointed all I have now is a charger that doesn't work.\n",
      "3) This is the first phone I've had that has been so cheaply made.\n",
      "4) I've also had problems with the phone reading the memory card in which I always turn it on and then off again.\n",
      "------------------------------------------------------------------------------------\n",
      "Reviews that resemble to the sentence : \n",
      "It doesn't make you look cool.\n",
      "------------------------------------------------------------------------------------\n",
      "0) It doesn't make you look cool.\n",
      "1) If you see it, you should probably just leave it on the shelf.  \n",
      "2) Seriously, it's not worth wasting your, or your kid's time on.  \n",
      "3) You'll love how thin it is.\n",
      "4) You can't beat the price on these.\n",
      "------------------------------------------------------------------------------------\n",
      "Reviews that resemble to the sentence : \n",
      "After my phone got to be about a year old, it's been slowly breaking despite much care on my part.\n",
      "------------------------------------------------------------------------------------\n",
      "0) After my phone got to be about a year old, it's been slowly breaking despite much care on my part.\n",
      "1) It is practically useless and did not add any kind of boost to my reception after I bought it.\n",
      "2) i got this phone around the end of may and i'm completely unhappy with it.\n",
      "3) I've also had problems with the phone reading the memory card in which I always turn it on and then off again.\n",
      "4) I put the latest OS on it (v1.15g), and it now likes to slow to a crawl and lock up every once in a while.\n",
      "------------------------------------------------------------------------------------\n",
      "Reviews that resemble to the sentence : \n",
      "The only thing that disappoint me is the infra red port (irda).\n",
      "------------------------------------------------------------------------------------\n",
      "0) The only thing that disappoint me is the infra red port (irda).\n",
      "1) The biggest complaint I have is, the battery drains superfast.\n",
      "2) That's a huge design flaw (unless I'm not using it correctly, which I don't think is the case).\n",
      "3) I have yet to run this new battery below two bars and that's three days without charging.\n",
      "4) In conclusion, I will not bother with this movie because a volcano in Los Angeles is nothing but nonsense.  \n",
      "------------------------------------------------------------------------------------\n",
      "Reviews that resemble to the sentence : \n",
      "It quit working after I'd used it for about 18 months, so I just purchased another one because this is the best headset I've ever owned.\n",
      "------------------------------------------------------------------------------------\n",
      "0) It quit working after I'd used it for about 18 months, so I just purchased another one because this is the best headset I've ever owned.\n",
      "1) I ordered this for sony Ericsson W810i but I think it only worked once (thats when I first used it).\n",
      "2) It's been my choice headset for years.Great sound; good volume; good noise cancellation.\n",
      "3) I've bought $5 wired headphones that sound better than these.\n",
      "4) It's uncomfortable and the sound quality is quite poor compared with the phone (Razr) or with my previous wired headset (that plugged into an LG).\n",
      "------------------------------------------------------------------------------------\n",
      "Reviews that resemble to the sentence : \n",
      "Unfortunately it did not work.\n",
      "------------------------------------------------------------------------------------\n",
      "0) Unfortunately it did not work.\n",
      "1) It worked very well.\n",
      "2) The charger arrived within the promised timeframe, but it did not work.\n",
      "3) The plug did not work very well.\n",
      "4) THAT one didn't work either.\n",
      "------------------------------------------------------------------------------------\n",
      "Reviews that resemble to the sentence : \n",
      "Excellent bluetooth headset.\n",
      "------------------------------------------------------------------------------------\n",
      "0) Excellent bluetooth headset.\n",
      "1) Excellent starter wireless headset.\n",
      "2) Excellent dual-purpose headset.\n",
      "3) Great Earpiece.\n",
      "4) Excellent wallet type phone case.\n"
     ]
    }
   ],
   "source": [
    "# choose some query sentences\n",
    "sentence_id_list = [5, 20, 70, 85, 92, 12, 521, 100, 712]\n",
    "\n",
    "# use K-nearest neighbors to find the 5 reviews that most closely resemble the query review\n",
    "for sentence_id in sentence_id_list:\n",
    "    query_QF = tr_embeddings_ND[sentence_id][np.newaxis, :]\n",
    "    _, nearest_ids_per_query = calc_k_nearest_neighbors(tr_embeddings_ND, query_QF, K=5)\n",
    "    nearest_ids = nearest_ids_per_query[0]\n",
    "\n",
    "    print('------------------------------------------------------------------------------------')\n",
    "    print('Reviews that resemble to the sentence : \\n%s'%tr_text_list[sentence_id])\n",
    "    print('------------------------------------------------------------------------------------')\n",
    "    for ii, idx in enumerate(nearest_ids):\n",
    "        print('%d) %s'%(ii, tr_text_list[idx]))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
