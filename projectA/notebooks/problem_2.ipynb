{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To train our model, we would split our data "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the data processing step for this part, we will repeat everything in part one \n",
    "of the problem but instead of unigram approach in part one, we will using the \n",
    "bigram method. We also reduced the minimum number of documents a word must occur in\n",
    "to be considered to one when processing the bag of words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary size: 21237\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import GridSearchCV, train_test_split\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix, roc_auc_score\n",
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "\n",
    "x_train_df = pd.read_csv('../data_reviews/x_train.csv')\n",
    "y_train_df = pd.read_csv('../data_reviews/y_train.csv')\n",
    "x_train_unshufled = x_train_df['text'].values.tolist()\n",
    "y = y_train_df['is_positive_sentiment'].values\n",
    "\n",
    "X_train, y_train = shuffle(x_train_unshufled, y, random_state=42)\n",
    "\n",
    "vectorizer = CountVectorizer(ngram_range=(1, 2))\n",
    "X = vectorizer.fit_transform(X_train)  # 'documents' is your input text data\n",
    "\n",
    "# Print the size of the vocabulary\n",
    "print(\"Vocabulary size:\", len(vectorizer.vocabulary_))\n",
    "\n",
    "pipeline = Pipeline([\n",
    "    ('vectorizer', vectorizer),\n",
    "    ('tfidf', TfidfTransformer(smooth_idf=True, use_idf=True, sublinear_tf=True)),\n",
    "    ('classifier', LogisticRegression(max_iter=1000, class_weight='balanced'))\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer\n",
    "# from sklearn.pipeline import Pipeline\n",
    "# from sklearn.linear_model import LogisticRegression\n",
    "# from sklearn.model_selection import GridSearchCV, train_test_split\n",
    "# from sklearn.metrics import accuracy_score, classification_report, confusion_matrix, roc_auc_score\n",
    "# import pandas as pd\n",
    "# from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "# from sklearn.utils import shuffle\n",
    "# from sklearn.svm import SVC\n",
    "\n",
    "\n",
    "# x_train_df = pd.read_csv('../data_reviews/x_train.csv')\n",
    "# y_train_df = pd.read_csv('../data_reviews/y_train.csv')\n",
    "# x_train_unshufled = x_train_df['text'].values.tolist()\n",
    "# y = y_train_df['is_positive_sentiment'].values\n",
    "\n",
    "# X_train, y_train = shuffle(x_train_unshufled, y, random_state=42)\n",
    "\n",
    "# X_train, X_test, y_train, y_test = train_test_split(X_train, y_train, test_size=0.2, random_state=42)\n",
    "\n",
    "\n",
    "\n",
    "# vectorizer = CountVectorizer(ngram_range=(1, 2))\n",
    "\n",
    "# pipeline = Pipeline([\n",
    "#     ('vectorizer', vectorizer),\n",
    "#     ('tfidf', TfidfTransformer(smooth_idf=True, use_idf=True, sublinear_tf=True)),\n",
    "#     ('classifier', SVC(probability=True, class_weight='balanced'))\n",
    "# ])\n",
    "\n",
    "# param_grid = {\n",
    "#     'classifier__C': [0.1, 1, 10, 100],  # Regularization parameter\n",
    "#     'classifier__kernel': ['linear', 'rbf', 'poly'],  # Kernel types\n",
    "#     'classifier__gamma': ['scale', 'auto'],  # Gamma for RBF and poly kernels\n",
    "# }\n",
    "\n",
    "# grid_search = GridSearchCV(pipeline, param_grid, cv=5, scoring='roc_auc', verbose=1, n_jobs=-1)\n",
    "\n",
    "# grid_search.fit(X_train, y_train)\n",
    "\n",
    "\n",
    "# # Best parameters\n",
    "# print(f\"Best params: {grid_search.best_params_}\")\n",
    "\n",
    "# # Predictions on the test set\n",
    "# y_pred = grid_search.best_estimator_.predict(X_test)\n",
    "# y_pred_prob = grid_search.best_estimator_.predict_proba(X_test)[:, 1]\n",
    "\n",
    "# # Evaluate performance\n",
    "# print(f\"Accuracy: {accuracy_score(y_test, y_pred)}\")\n",
    "# print(f\"Classification Report:\\n {classification_report(y_test, y_pred)}\")\n",
    "# print(f\"Confusion Matrix:\\n {confusion_matrix(y_test, y_pred)}\")\n",
    "\n",
    "# # AUC score\n",
    "# auc = roc_auc_score(y_test, y_pred_prob)\n",
    "# print(f'AUC: {auc:.4f}')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/manuelpena/micromamba/envs/cs135_env/lib/python3.10/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/Users/manuelpena/micromamba/envs/cs135_env/lib/python3.10/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/Users/manuelpena/micromamba/envs/cs135_env/lib/python3.10/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/Users/manuelpena/micromamba/envs/cs135_env/lib/python3.10/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/Users/manuelpena/micromamba/envs/cs135_env/lib/python3.10/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/Users/manuelpena/micromamba/envs/cs135_env/lib/python3.10/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/Users/manuelpena/micromamba/envs/cs135_env/lib/python3.10/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/Users/manuelpena/micromamba/envs/cs135_env/lib/python3.10/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/Users/manuelpena/micromamba/envs/cs135_env/lib/python3.10/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/Users/manuelpena/micromamba/envs/cs135_env/lib/python3.10/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/Users/manuelpena/micromamba/envs/cs135_env/lib/python3.10/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/Users/manuelpena/micromamba/envs/cs135_env/lib/python3.10/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/Users/manuelpena/micromamba/envs/cs135_env/lib/python3.10/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/Users/manuelpena/micromamba/envs/cs135_env/lib/python3.10/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/Users/manuelpena/micromamba/envs/cs135_env/lib/python3.10/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/Users/manuelpena/micromamba/envs/cs135_env/lib/python3.10/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/Users/manuelpena/micromamba/envs/cs135_env/lib/python3.10/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/Users/manuelpena/micromamba/envs/cs135_env/lib/python3.10/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/Users/manuelpena/micromamba/envs/cs135_env/lib/python3.10/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/Users/manuelpena/micromamba/envs/cs135_env/lib/python3.10/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/Users/manuelpena/micromamba/envs/cs135_env/lib/python3.10/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/Users/manuelpena/micromamba/envs/cs135_env/lib/python3.10/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/Users/manuelpena/micromamba/envs/cs135_env/lib/python3.10/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/Users/manuelpena/micromamba/envs/cs135_env/lib/python3.10/site-packages/sklearn/model_selection/_validation.py:378: FitFailedWarning: \n",
      "375 fits failed out of a total of 750.\n",
      "The score on these train-test partitions for these parameters will be set to nan.\n",
      "If these failures are not expected, you can try to debug them by setting error_score='raise'.\n",
      "\n",
      "Below are more details about the failures:\n",
      "--------------------------------------------------------------------------------\n",
      "125 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/manuelpena/micromamba/envs/cs135_env/lib/python3.10/site-packages/sklearn/model_selection/_validation.py\", line 686, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"/Users/manuelpena/micromamba/envs/cs135_env/lib/python3.10/site-packages/sklearn/pipeline.py\", line 405, in fit\n",
      "    self._final_estimator.fit(Xt, y, **fit_params_last_step)\n",
      "  File \"/Users/manuelpena/micromamba/envs/cs135_env/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py\", line 1162, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"/Users/manuelpena/micromamba/envs/cs135_env/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py\", line 54, in _check_solver\n",
      "    raise ValueError(\n",
      "ValueError: Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "125 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/manuelpena/micromamba/envs/cs135_env/lib/python3.10/site-packages/sklearn/model_selection/_validation.py\", line 686, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"/Users/manuelpena/micromamba/envs/cs135_env/lib/python3.10/site-packages/sklearn/pipeline.py\", line 405, in fit\n",
      "    self._final_estimator.fit(Xt, y, **fit_params_last_step)\n",
      "  File \"/Users/manuelpena/micromamba/envs/cs135_env/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py\", line 1162, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"/Users/manuelpena/micromamba/envs/cs135_env/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py\", line 54, in _check_solver\n",
      "    raise ValueError(\n",
      "ValueError: Solver lbfgs supports only 'l2' or 'none' penalties, got elasticnet penalty.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "125 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/manuelpena/micromamba/envs/cs135_env/lib/python3.10/site-packages/sklearn/model_selection/_validation.py\", line 686, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"/Users/manuelpena/micromamba/envs/cs135_env/lib/python3.10/site-packages/sklearn/pipeline.py\", line 405, in fit\n",
      "    self._final_estimator.fit(Xt, y, **fit_params_last_step)\n",
      "  File \"/Users/manuelpena/micromamba/envs/cs135_env/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py\", line 1291, in fit\n",
      "    fold_coefs_ = Parallel(n_jobs=self.n_jobs, verbose=self.verbose, prefer=prefer)(\n",
      "  File \"/Users/manuelpena/micromamba/envs/cs135_env/lib/python3.10/site-packages/sklearn/utils/parallel.py\", line 63, in __call__\n",
      "    return super().__call__(iterable_with_config)\n",
      "  File \"/Users/manuelpena/micromamba/envs/cs135_env/lib/python3.10/site-packages/joblib/parallel.py\", line 1918, in __call__\n",
      "    return output if self.return_generator else list(output)\n",
      "  File \"/Users/manuelpena/micromamba/envs/cs135_env/lib/python3.10/site-packages/joblib/parallel.py\", line 1847, in _get_sequential_output\n",
      "    res = func(*args, **kwargs)\n",
      "  File \"/Users/manuelpena/micromamba/envs/cs135_env/lib/python3.10/site-packages/sklearn/utils/parallel.py\", line 123, in __call__\n",
      "    return self.function(*args, **kwargs)\n",
      "  File \"/Users/manuelpena/micromamba/envs/cs135_env/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py\", line 521, in _logistic_regression_path\n",
      "    alpha = (1.0 / C) * (1 - l1_ratio)\n",
      "TypeError: unsupported operand type(s) for -: 'int' and 'NoneType'\n",
      "\n",
      "  warnings.warn(some_fits_failed_message, FitFailedWarning)\n",
      "/Users/manuelpena/micromamba/envs/cs135_env/lib/python3.10/site-packages/sklearn/model_selection/_search.py:952: UserWarning: One or more of the test scores are non-finite: [       nan        nan        nan        nan        nan 0.5\n",
      " 0.5        0.5        0.5        0.5        0.75056944 0.82417361\n",
      " 0.84325868 0.86076389 0.86488542 0.75059375 0.82420833 0.84319271\n",
      " 0.86074653 0.86488542        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan 0.71244444\n",
      " 0.66146007 0.64763715 0.55982986 0.55982292 0.7575625  0.83647569\n",
      " 0.85363368 0.86743056 0.87062847 0.75773958 0.83652083 0.85362326\n",
      " 0.86738194 0.87060764        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan 0.76376563\n",
      " 0.85965104 0.86701736 0.84106771 0.82953819 0.75885417 0.85424653\n",
      " 0.87382465 0.89014931 0.89187847 0.75887847 0.85421181 0.87384896\n",
      " 0.89018056 0.89190625        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan 0.75752431\n",
      " 0.83958507 0.86193403 0.89817361 0.89932465 0.75708681 0.84751389\n",
      " 0.8699566  0.90296875 0.90693056 0.75709028 0.84753125 0.87003993\n",
      " 0.90299653 0.90697222        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan 0.75690278\n",
      " 0.81849826 0.82883507 0.9027066  0.90457292 0.75704861 0.82964931\n",
      " 0.84369271 0.90314931 0.90969792 0.75698264 0.82968056 0.84396007\n",
      " 0.90320139 0.90969792        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan]\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-4 {color: black;background-color: white;}#sk-container-id-4 pre{padding: 0;}#sk-container-id-4 div.sk-toggleable {background-color: white;}#sk-container-id-4 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-4 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-4 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-4 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-4 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-4 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-4 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-4 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-4 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-4 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-4 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-4 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-4 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-4 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-4 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-4 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-4 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-4 div.sk-item {position: relative;z-index: 1;}#sk-container-id-4 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-4 div.sk-item::before, #sk-container-id-4 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-4 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-4 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-4 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-4 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-4 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-4 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-4 div.sk-label-container {text-align: center;}#sk-container-id-4 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-4 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-4\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>GridSearchCV(cv=5,\n",
       "             estimator=Pipeline(steps=[(&#x27;vectorizer&#x27;,\n",
       "                                        CountVectorizer(ngram_range=(1, 2))),\n",
       "                                       (&#x27;tfidf&#x27;,\n",
       "                                        TfidfTransformer(sublinear_tf=True)),\n",
       "                                       (&#x27;classifier&#x27;,\n",
       "                                        LogisticRegression(class_weight=&#x27;balanced&#x27;,\n",
       "                                                           max_iter=1000))]),\n",
       "             param_grid={&#x27;classifier__C&#x27;: [0.01, 0.1, 1, 10, 100],\n",
       "                         &#x27;classifier__penalty&#x27;: [&#x27;l1&#x27;, &#x27;l2&#x27;, &#x27;elasticnet&#x27;],\n",
       "                         &#x27;classifier__solver&#x27;: [&#x27;lbfgs&#x27;, &#x27;saga&#x27;],\n",
       "                         &#x27;vectorizer__max_features&#x27;: [100, 500, 1000, 10000,\n",
       "                                                      100000]},\n",
       "             scoring=&#x27;roc_auc&#x27;)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item sk-dashed-wrapped\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-16\" type=\"checkbox\" ><label for=\"sk-estimator-id-16\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">GridSearchCV</label><div class=\"sk-toggleable__content\"><pre>GridSearchCV(cv=5,\n",
       "             estimator=Pipeline(steps=[(&#x27;vectorizer&#x27;,\n",
       "                                        CountVectorizer(ngram_range=(1, 2))),\n",
       "                                       (&#x27;tfidf&#x27;,\n",
       "                                        TfidfTransformer(sublinear_tf=True)),\n",
       "                                       (&#x27;classifier&#x27;,\n",
       "                                        LogisticRegression(class_weight=&#x27;balanced&#x27;,\n",
       "                                                           max_iter=1000))]),\n",
       "             param_grid={&#x27;classifier__C&#x27;: [0.01, 0.1, 1, 10, 100],\n",
       "                         &#x27;classifier__penalty&#x27;: [&#x27;l1&#x27;, &#x27;l2&#x27;, &#x27;elasticnet&#x27;],\n",
       "                         &#x27;classifier__solver&#x27;: [&#x27;lbfgs&#x27;, &#x27;saga&#x27;],\n",
       "                         &#x27;vectorizer__max_features&#x27;: [100, 500, 1000, 10000,\n",
       "                                                      100000]},\n",
       "             scoring=&#x27;roc_auc&#x27;)</pre></div></div></div><div class=\"sk-parallel\"><div class=\"sk-parallel-item\"><div class=\"sk-item\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-17\" type=\"checkbox\" ><label for=\"sk-estimator-id-17\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">estimator: Pipeline</label><div class=\"sk-toggleable__content\"><pre>Pipeline(steps=[(&#x27;vectorizer&#x27;, CountVectorizer(ngram_range=(1, 2))),\n",
       "                (&#x27;tfidf&#x27;, TfidfTransformer(sublinear_tf=True)),\n",
       "                (&#x27;classifier&#x27;,\n",
       "                 LogisticRegression(class_weight=&#x27;balanced&#x27;, max_iter=1000))])</pre></div></div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-18\" type=\"checkbox\" ><label for=\"sk-estimator-id-18\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">CountVectorizer</label><div class=\"sk-toggleable__content\"><pre>CountVectorizer(ngram_range=(1, 2))</pre></div></div></div><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-19\" type=\"checkbox\" ><label for=\"sk-estimator-id-19\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">TfidfTransformer</label><div class=\"sk-toggleable__content\"><pre>TfidfTransformer(sublinear_tf=True)</pre></div></div></div><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-20\" type=\"checkbox\" ><label for=\"sk-estimator-id-20\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">LogisticRegression</label><div class=\"sk-toggleable__content\"><pre>LogisticRegression(class_weight=&#x27;balanced&#x27;, max_iter=1000)</pre></div></div></div></div></div></div></div></div></div></div></div></div>"
      ],
      "text/plain": [
       "GridSearchCV(cv=5,\n",
       "             estimator=Pipeline(steps=[('vectorizer',\n",
       "                                        CountVectorizer(ngram_range=(1, 2))),\n",
       "                                       ('tfidf',\n",
       "                                        TfidfTransformer(sublinear_tf=True)),\n",
       "                                       ('classifier',\n",
       "                                        LogisticRegression(class_weight='balanced',\n",
       "                                                           max_iter=1000))]),\n",
       "             param_grid={'classifier__C': [0.01, 0.1, 1, 10, 100],\n",
       "                         'classifier__penalty': ['l1', 'l2', 'elasticnet'],\n",
       "                         'classifier__solver': ['lbfgs', 'saga'],\n",
       "                         'vectorizer__max_features': [100, 500, 1000, 10000,\n",
       "                                                      100000]},\n",
       "             scoring='roc_auc')"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "param_grid = {\n",
    "    'classifier__solver': ['lbfgs', 'saga'],\n",
    "    'vectorizer__max_features': [100, 500, 1000, 10000, 100000],\n",
    "    'classifier__C': [0.01, 0.1, 1, 10, 100],\n",
    "    'classifier__penalty': ['l1', 'l2', 'elasticnet']\n",
    "}\n",
    "\n",
    "grid_search = GridSearchCV(\n",
    "    pipeline,\n",
    "    param_grid,\n",
    "    cv=5,\n",
    "    scoring='roc_auc'\n",
    ")\n",
    "\n",
    "# param_grid = {\n",
    "#     'classifier__C': [0.1, 1, 10],  # Regularization parameter\n",
    "#     'classifier__kernel': ['linear', 'rbf'],  # Linear and RBF kernels\n",
    "#     'classifier__gamma': ['scale', 'auto']  # Gamma parameter for RBF kernel\n",
    "# }\n",
    "\n",
    "# Set up the grid search\n",
    "# grid_search = GridSearchCV(\n",
    "#     pipeline, \n",
    "#     param_grid, \n",
    "#     cv=5, \n",
    "#     scoring='roc_auc', \n",
    "#     verbose=1, \n",
    "#     n_jobs=-1\n",
    "# )\n",
    "\n",
    "grid_search.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00      1200\n",
      "           1       1.00      1.00      1.00      1200\n",
      "\n",
      "    accuracy                           1.00      2400\n",
      "   macro avg       1.00      1.00      1.00      2400\n",
      "weighted avg       1.00      1.00      1.00      2400\n",
      "\n",
      "[[1200    0]\n",
      " [   0 1200]]\n",
      "AUC: 1.0000\n"
     ]
    }
   ],
   "source": [
    "y_pred = grid_search.best_estimator_.predict(X_train)\n",
    "\n",
    "print(classification_report(y_train, y_pred))\n",
    "print(confusion_matrix(y_train, y_pred))\n",
    "\n",
    "y_pred_prob = grid_search.best_estimator_.predict_proba(X_train)[:, 1]\n",
    "\n",
    "auc = roc_auc_score(y_train, y_pred_prob)\n",
    "\n",
    "print(f'AUC: {auc:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Probabilistic predictions saved to '../data_reviews/yproba1_test_part2.txt'.\n"
     ]
    }
   ],
   "source": [
    "# Load the test data\n",
    "x_test_df = pd.read_csv('../data_reviews/x_test.csv')\n",
    "\n",
    "# Get the predicted probabilities for the positive class\n",
    "y_test_pred_prob = grid_search.best_estimator_.predict_proba(x_test_df['text'])[:, 1]\n",
    "\n",
    "# Save the probabilities to a plain-text file\n",
    "with open('../data_reviews/yproba1_test_part2.txt', 'w') as f:\n",
    "    for prob in y_test_pred_prob:\n",
    "        f.write(f\"{prob:.6f}\\n\")  # Formatting to six decimal places\n",
    "\n",
    "print(\"Probabilistic predictions saved to '../data_reviews/yproba1_test_part2.txt'.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cs135_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
